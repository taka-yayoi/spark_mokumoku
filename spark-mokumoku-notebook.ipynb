{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4546f9a-40b8-461b-a8b5-a13b4898e751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Apache Sparkもくもく会ハンズオンノートブック\n",
    "\n",
    "## 🎯 今日の目標\n",
    "このもくもく会では、Apache Sparkの基礎を実際に手を動かしながら学んでいきます。大規模データ処理の世界への第一歩を踏み出しましょう！\n",
    "\n",
    "### 📋 事前準備\n",
    "- [Databricks Free Edition](https://qiita.com/taka_yayoi/items/33e9cfa7ca9ca9febe72)にログイン済み\n",
    "- [サーバレスコンピュート](https://docs.databricks.com/aws/ja/compute/serverless/)が起動済み\n",
    "- このノートブックをインポート済み\n",
    "\n",
    "### 🚀 学習の流れ\n",
    "1. Sparkの基本概念を理解\n",
    "2. データフレームの操作を習得\n",
    "3. 実データでの分析を実践\n",
    "4. SQLとの連携を学習\n",
    "5. Unity Catalogとの統合を理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58013ef0-ad8f-45d8-aa74-03353747eafa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📚 第1部: Apache Sparkとは\n",
    "\n",
    "### Sparkの誕生ストーリー\n",
    "2009年、カリフォルニア大学バークレー校で誕生したApache Sparkは、ビッグデータ処理に革命をもたらしました。Databricksの創業者たちが開発したこのフレームワークは、従来のHadoop MapReduceと比較して、**中間結果をメモリーに保持することで100倍以上の高速化**を実現しました。\n",
    "\n",
    "### 🐼 PandasとSparkの違い\n",
    "\n",
    "多くの方が使い慣れているPandasと比較してSparkを理解しましょう：\n",
    "\n",
    "| 特徴 | Pandas | Apache Spark |\n",
    "|------|--------|-------------|\n",
    "| **処理方式** | 単一マシン（シングルスレッド） | 分散処理（複数マシン・並列処理） |\n",
    "| **データサイズ** | メモリに収まる範囲（通常数GB） | メモリを超える大規模データ（TB〜PB） |\n",
    "| **処理速度** | 小規模データでは高速 | 大規模データで圧倒的に高速 |\n",
    "| **実行タイミング** | 即座に実行（Eager Evaluation） | 遅延評価（Lazy Evaluation） |\n",
    "| **データ構造** | DataFrame | DataFrame（分散） |\n",
    "| **スケーラビリティ** | 垂直スケール（マシンスペック依存） | 水平スケール（ノード追加で対応） |\n",
    "| **エラー処理** | エラー時は最初から | 障害耐性あり（自動リトライ） |\n",
    "\n",
    "![](img/spark_pandas.png)\n",
    "\n",
    "### なぜSparkを学ぶのか？\n",
    "- **🚀 スピード**: メモリベースの処理により、大規模データを高速に処理\n",
    "- **🎯 使いやすさ**: Pandasに似たDataFrame APIで学習曲線が緩やか\n",
    "- **🔧 モジュール性**: SQL、機械学習、ストリーミング処理など多様な用途に対応\n",
    "- **📈 拡張性**: データ量の増加に柔軟に対応可能\n",
    "\n",
    "### 🔍 これから実行するコード\n",
    "最初に、Databricksで自動的に作成されているSparkセッションを確認します。\n",
    "Sparkセッションは、Sparkの全機能への入口となる重要なオブジェクトです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf249d4-701f-4382-9710-6c277c540877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sparkセッションの確認\n",
    "# Databricksでは起動時に自動的に`spark`変数が作成されています\n",
    "# これがSparkへのエントリーポイントとなります\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e9edfb-8095-4144-83e5-cd81baf3759a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 📊 Sparkのバージョンと設定を確認\n",
    "現在使用しているSparkの環境情報を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f145795-4967-4938-9898-77651ef19dcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sparkのバージョンを確認\n",
    "# サーバレスコンピュートでは最新のSparkバージョンが使用されます\n",
    "print(f\"Sparkバージョン: {spark.version}\")\n",
    "\n",
    "# 現在のカタログとデータベースを確認\n",
    "print(f\"現在のカタログ: {spark.sql('SELECT current_catalog()').first()[0]}\")\n",
    "print(f\"現在のデータベース: {spark.sql('SELECT current_database()').first()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a0488e7-4fcc-4a6b-b3f1-eedd89edd056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🏗️ 第2部: Sparkの基本アーキテクチャを詳しく理解する\n",
    "\n",
    "### 📐 Sparkクラスターの構成要素\n",
    "\n",
    "![](img/spark_architecture.png)\n",
    "\n",
    "Sparkは**分散処理システム**として設計されており、以下の主要コンポーネントから構成されています：\n",
    "\n",
    "#### 1. ドライバー（Driver）\n",
    "- **役割**: アプリケーション全体の司令塔\n",
    "- **責任**:\n",
    "  - ユーザーコードをタスクに変換\n",
    "  - タスクのスケジューリング\n",
    "  - エグゼキューターの監視\n",
    "  - 実行計画の最適化\n",
    "\n",
    "#### 2. エグゼキューター（Executor）\n",
    "- **役割**: 実際のデータ処理を実行\n",
    "- **責任**:\n",
    "  - タスクの実行\n",
    "  - データの保持（メモリまたはディスク）\n",
    "  - 中間結果の保存\n",
    "  - ドライバーへの結果返却\n",
    "\n",
    "#### 3. クラスターマネージャー\n",
    "- **役割**: リソースの管理と割り当て\n",
    "- **種類**: Databricks（サーバレス）、YARN、Mesos、Kubernetes等\n",
    "\n",
    "### 🔄 処理の流れ\n",
    "\n",
    "```\n",
    "[ユーザーコード] \n",
    "    ↓\n",
    "[ドライバー] → 論理実行計画 → 物理実行計画 → タスク生成\n",
    "    ↓\n",
    "[クラスターマネージャー] → リソース割り当て\n",
    "    ↓\n",
    "[エグゼキューター群] → 並列実行\n",
    "    ↓\n",
    "[結果の集約]\n",
    "```\n",
    "\n",
    "### 🎯 重要な概念\n",
    "\n",
    "1. **パーティション**: データを分割した単位（並列処理の基本単位）\n",
    "2. **タスク**: パーティションに対する処理の単位\n",
    "3. **ステージ**: シャッフルで区切られたタスクのグループ\n",
    "4. **ジョブ**: アクションによって起動される全体の処理\n",
    "\n",
    "### 🔍 アーキテクチャの動作原理\n",
    "\n",
    "Sparkでデータ処理を実行する際の内部動作を理解しましょう：\n",
    "\n",
    "#### 例: 1000個の数値を処理する場合\n",
    "\n",
    "```python\n",
    "# データフレームを4つのパーティションで作成\n",
    "numbers_df = spark.range(0, 1000, 1, numPartitions=4)\n",
    "```\n",
    "\n",
    "このコードが実行されると、以下のような処理が行われます：\n",
    "\n",
    "1. **パーティション分割**\n",
    "   - 0-249: パーティション1 → エグゼキューター1で処理\n",
    "   - 250-499: パーティション2 → エグゼキューター2で処理\n",
    "   - 500-749: パーティション3 → エグゼキューター3で処理\n",
    "   - 750-999: パーティション4 → エグゼキューター4で処理\n",
    "\n",
    "2. **並列実行**\n",
    "   - 4つのタスクが同時に実行される\n",
    "   - 各エグゼキューターが独立して処理\n",
    "   - 処理時間は約1/4に短縮\n",
    "\n",
    "3. **結果の集約**\n",
    "   - 各エグゼキューターの結果をドライバーが収集\n",
    "   - 最終的な結果を生成\n",
    "\n",
    "#### サーバレス環境での最適化\n",
    "\n",
    "Databricksのサーバレスコンピュートでは：\n",
    "- パーティション数が自動的に最適化される\n",
    "- リソースが動的に割り当てられる\n",
    "- 負荷に応じてエグゼキューターが自動スケール\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c20113-553b-400f-8efc-877aaadccca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 📝 理解度チェック\n",
    "- Q1: ドライバーの主な役割は何でしょうか？\n",
    "- Q2: エグゼキューターは何を実行しますか？\n",
    "- Q3: パーティションとタスクの関係は？\n",
    "\n",
    "これらの質問に答えられるようになれば、Sparkアーキテクチャの基本を理解できています！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb34ede-89a1-4e6d-9208-21aebbbabd37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🎬 第3部: 最初のSparkデータフレームを作成\n",
    "\n",
    "### 📖 トランスフォーメーションとアクションの理解\n",
    "\n",
    "Sparkの処理は大きく2種類に分類されます：\n",
    "\n",
    "1. **トランスフォーメーション（Transformation）**\n",
    "   - データの変換処理を定義（まだ実行されない）\n",
    "   - 例: `select()`, `filter()`, `groupBy()`\n",
    "   \n",
    "2. **アクション（Action）**\n",
    "   - 実際に処理を実行して結果を返す\n",
    "   - 例: `count()`, `show()`, `collect()`\n",
    "\n",
    "![](img/transform_action.png)\n",
    "\n",
    "### Step 1: シンプルなデータフレームの作成\n",
    "まず、連番データを持つデータフレームを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "111a1995-816b-433a-98fe-21d1ab8a8526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 大規模データを想定して、1から100万までの数値を生成\n",
    "# range()メソッドは「トランスフォーメーション」\n",
    "# この時点ではデータは生成されず、処理の定義のみが作成されます\n",
    "first_df = spark.range(1000000)\n",
    "\n",
    "# データフレームの型を確認\n",
    "print(f\"データフレームの型: {type(first_df)}\")\n",
    "print(f\"データフレームのクラス: {first_df.__class__.__name__}\")\n",
    "\n",
    "# スキーマ（データ構造）を確認\n",
    "# この情報はメタデータとして保持されているため、データを読まずに表示できます\n",
    "print(f\"\\nスキーマ情報: {first_df}\")\n",
    "print(\"\\n詳細なスキーマ:\")\n",
    "first_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e6371d1-0f9a-45d1-bda4-f3a63b209938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: トランスフォーメーションのチェーン\n",
    "複数のトランスフォーメーションを連鎖させて、処理のパイプラインを構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ee20d1-df36-4599-b281-da7f586c3aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# selectExpr()を使って、SQL式で新しいカラムを作成\n",
    "# まだ実行されません - 処理の定義を追加するだけ\n",
    "doubled_df = first_df.selectExpr(\n",
    "    \"id as original_id\",           # 元のIDカラムをリネーム\n",
    "    \"(id * 2) as doubled_value\"     # IDを2倍にした新しいカラム\n",
    ")\n",
    "\n",
    "# さらにトランスフォーメーションを追加\n",
    "# メソッドチェーンで複数の処理を連結できます\n",
    "final_df = doubled_df.selectExpr(\n",
    "    \"original_id\",\n",
    "    \"doubled_value\",\n",
    "    \"(doubled_value / 1000) as divided_by_1000\",     # 1000で割る\n",
    "    \"(doubled_value % 100) as modulo_100\"            # 100で割った余り\n",
    ")\n",
    "\n",
    "# この時点でもまだ処理は実行されていません\n",
    "print(\"トランスフォーメーションのチェーンが定義されました\")\n",
    "print(\"実際の処理はアクションが呼ばれるまで実行されません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "288b4c99-7f67-436f-b451-bd9cf312fb09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: アクションで処理を実行\n",
    "アクションを呼び出すことで、定義した処理が実際に実行されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e70372-e005-494f-b088-dca3c565fc13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take()アクションで上位N件を取得\n",
    "# ここで初めてSparkが処理を実行します！\n",
    "print(\"🎯 処理を実行中...\")\n",
    "results = final_df.take(10)  # 上位10件を取得\n",
    "\n",
    "# 結果を見やすく表示\n",
    "print(\"\\n📊 処理結果の最初の10件:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'元のID':>10} | {'2倍の値':>10} | {'1000で割った値':>15} | {'100の余り':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for row in results:\n",
    "    print(f\"{row['original_id']:10d} | \"\n",
    "          f\"{row['doubled_value']:10d} | \"\n",
    "          f\"{row['divided_by_1000']:15.2f} | \"\n",
    "          f\"{row['modulo_100']:10d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa7eddf-323c-4b74-b6b1-0132bfc71273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 🎨 Databricksの可視化機能を活用\n",
    "`display()`関数を使うと、結果をインタラクティブに表示できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f113e2a-dd3b-48f3-a720-b5b8acdc6e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display()もアクションとして動作します\n",
    "# 上位100件を表示（大量データの場合は自動的に制限されます）\n",
    "print(\"📈 インタラクティブな表示:\")\n",
    "display(final_df.limit(100))  # limit()はトランスフォーメーション、display()がアクション"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a1134a1-4b27-4828-9fae-2d350411ae44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📊 第4部: 実データで学ぶデータ処理\n",
    "\n",
    "### 💎 ダイヤモンドデータセットの紹介\n",
    "ここからは、実際のデータセットを使って実践的なデータ処理を学びます。\n",
    "使用するのは、約54,000個のダイヤモンドの品質と価格に関するデータです。\n",
    "\n",
    "![](img/pipeline.png)\n",
    "\n",
    "### データの読み込み\n",
    "CSV形式のファイルをSparkデータフレームに読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e19fb6e-bfde-45ce-9141-8dd63a3e720d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricksに事前に用意されているサンプルデータのパス\n",
    "data_path = \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\"\n",
    "\n",
    "# CSVファイルの読み込み\n",
    "# spark.read を使用してデータを読み込みます\n",
    "diamonds = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")         # 1行目をヘッダーとして扱う\n",
    "    .option(\"inferSchema\", \"true\")    # データ型を自動推論\n",
    "    .load(data_path)                   # ファイルパスを指定して読み込み\n",
    ")\n",
    "\n",
    "# データセットの基本情報を表示\n",
    "print(\"📊 データセットの概要:\")\n",
    "print(f\"  総レコード数: {diamonds.count():,} 件\")\n",
    "print(f\"  カラム数: {len(diamonds.columns)} 個\")\n",
    "print(\"\\n📝 カラム一覧:\")\n",
    "for i, col in enumerate(diamonds.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc86c950-031f-4cf0-99c7-e6e10a06fc84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### データの探索\n",
    "データの中身を確認して、どのような情報が含まれているか理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5199526-365a-4b10-941d-20f67f3963bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# show()メソッドで最初の数行を表示\n",
    "# truncate=Falseで文字列が切れないように表示\n",
    "print(\"💎 ダイヤモンドデータの最初の5行:\")\n",
    "diamonds.show(5, truncate=False)\n",
    "\n",
    "# 各カラムの説明\n",
    "print(\"\\n📖 カラムの説明:\")\n",
    "print(\"  - carat: カラット（重さ）\")\n",
    "print(\"  - cut: カットの品質（Fair, Good, Very Good, Premium, Ideal）\")\n",
    "print(\"  - color: 色のグレード（D=最高 から J=最低）\")\n",
    "print(\"  - clarity: 透明度（I1=最低, SI2, SI1, VS2, VS1, VVS2, VVS1, IF=最高）\")\n",
    "print(\"  - price: 価格（USドル）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd2b2d9-f04c-42e0-aaad-79b74dcb08e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# データ型（スキーマ）の詳細を確認\n",
    "print(\"📋 データ型の詳細:\")\n",
    "diamonds.printSchema()\n",
    "\n",
    "# 各データ型の説明\n",
    "print(\"\\n💡 データ型の説明:\")\n",
    "print(\"  - string: 文字列型（カテゴリカルデータ）\")\n",
    "print(\"  - double: 浮動小数点型（連続値）\")\n",
    "print(\"  - integer: 整数型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8833bf-09dd-433d-a67e-5e4a4bae5b63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### データクレンジング\n",
    "分析しやすいようにデータを整形します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96294c65-e0e7-4071-806e-5d9029598dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# カラム名の変更とデータ型の調整\n",
    "diamonds_clean = (\n",
    "    diamonds\n",
    "    .withColumnRenamed(\"_c0\", \"index\")           # 最初のカラムをindexに変更\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"integer\"))  # priceを整数型に変換\n",
    "    .withColumn(\"carat\", col(\"carat\").cast(\"float\"))    # caratを浮動小数点型に変換\n",
    ")\n",
    "\n",
    "# クレンジング結果を確認\n",
    "print(\"✨ クレンジング後のデータ:\")\n",
    "display(diamonds_clean.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1746d022-9077-4795-acae-fa64c8fe389b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NULL値の確認\n",
    "# 実データでは欠損値の確認が重要です\n",
    "from pyspark.sql.functions import count, when, isnan\n",
    "\n",
    "print(\"🔍 各カラムのNULL値をチェック:\")\n",
    "\n",
    "# 各カラムのNULL値をカウントする処理の詳細\n",
    "# 1. diamonds_clean.columns で全カラム名のリストを取得\n",
    "# 2. 各カラムに対してcount(when(...))を適用\n",
    "#    - when(col(c).isNull(), c): カラムcがNULLの場合、カラム名を返す\n",
    "#    - count(): NULLでない値（つまり上記whenで返されたカラム名）をカウント\n",
    "#    - alias(c): 結果のカラム名を元のカラム名と同じにする\n",
    "# 3. select([...])で全カラムのNULLカウントを一度に取得\n",
    "\n",
    "null_counts = diamonds_clean.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in diamonds_clean.columns\n",
    "])\n",
    "\n",
    "# 結果を表示\n",
    "# 各カラムの下に表示される数値がNULL値の個数\n",
    "# 0が表示されていれば、そのカラムにはNULL値が存在しない\n",
    "display(null_counts)\n",
    "\n",
    "# 追加情報: isnan()関数について\n",
    "# isnan()は数値カラムのNaN（Not a Number）をチェックする関数\n",
    "# このデータセットでは文字列カラムも含まれているため、\n",
    "# isNull()を使用してNULL値をチェックしています\n",
    "\n",
    "print(\"\\n✅ このデータセットには欠損値がないことが確認できました\")\n",
    "print(\"   （全カラムでNULL値のカウントが0）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0396fc14-55cf-4d92-a500-19605a907bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📈 第5部: データ分析の実践\n",
    "\n",
    "### 基本的な集計処理\n",
    "GroupByとAggregation関数を使って、データを集計します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eecd9360-9b01-4441-991c-dd62fd6a747f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 必要な集計関数をインポート\n",
    "from pyspark.sql.functions import avg, max, min, stddev, round, count\n",
    "\n",
    "# カット品質ごとの価格統計を計算\n",
    "cut_analysis = (\n",
    "    diamonds_clean\n",
    "    .groupBy(\"cut\")                              # カット品質でグループ化\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"個数\"),                 # レコード数をカウント\n",
    "        round(avg(\"price\"), 2).alias(\"平均価格\"),  # 平均価格（小数点2桁）\n",
    "        round(min(\"price\"), 2).alias(\"最低価格\"),  # 最低価格\n",
    "        round(max(\"price\"), 2).alias(\"最高価格\"),  # 最高価格\n",
    "        round(stddev(\"price\"), 2).alias(\"価格の標準偏差\")  # 価格のばらつき\n",
    "    )\n",
    "    .orderBy(\"平均価格\", ascending=False)          # 平均価格の降順でソート\n",
    ")\n",
    "\n",
    "print(\"💎 カット品質別の価格分析:\")\n",
    "print(\"（注: 興味深いことに、'Ideal'カットが最も安い傾向があります）\")\n",
    "display(cut_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fedbc172-e7cd-4095-9596-76791af3f6d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 複雑な分析: カテゴリ分けと複合集計\n",
    "条件分岐を使ったカテゴリ分けと、複数の軸での集計を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb17036-c0b9-4159-a906-f374230d4da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IHdoZW4sIGNvbAoKIyDjgqvjg6njg4Pjg4jjgrXjgqTjgrrjgafjgqvjg4bjgrTjg6rliIbjgZEKIyB3aGVuKCnplqLmlbDjgafmnaHku7bliIblspDjgpLlrp/oo4UKIyDnlarlj7fjgpLku5jjgZHjgovjgZPjgajjgafjgIHooajnpLrmmYLjgavpganliIfjgarpoIbluo/jgafjgr3jg7zjg4jjgZXjgozjgovjgojjgYbjgavjgZnjgosKZGlhbW9uZHNfY2F0ZWdvcml6ZWQgPSBkaWFtb25kc19jbGVhbi53aXRoQ29sdW1uKAogICAgImNhcmF0X2NhdGVnb3J5IiwKICAgIHdoZW4oY29sKCJjYXJhdCIpIDwgMC41LCAiMV/lsI/vvIg8IDAuNWN077yJIikKICAgIC53aGVuKGNvbCgiY2FyYXQiKSA8IDEuMCwgIjJf5Lit77yIMC41LTEuMGN077yJIikKICAgIC53aGVuKGNvbCgiY2FyYXQiKSA8IDEuNSwgIjNf5aSn77yIMS4wLTEuNWN077yJIikKICAgIC5vdGhlcndpc2UoIjRf54m55aSn77yIPj0gMS41Y3TvvIkiKQopCgojIOOCq+ODhuOCtOODquOBqOOCq+ODg+ODiOOBrue1hOOBv+WQiOOCj+OBm+OBp+mbhuioiApjYXRlZ29yeV9hbmFseXNpcyA9ICgKICAgIGRpYW1vbmRzX2NhdGVnb3JpemVkCiAgICAuZ3JvdXBCeSgiY2FyYXRfY2F0ZWdvcnkiLCAiY3V0IikgICAgICAgICAgICAjIDLjgaTjga7ou7jjgafjgrDjg6vjg7zjg5fljJYKICAgIC5hZ2coCiAgICAgICAgY291bnQoIioiKS5hbGlhcygi5YCL5pWwIiksCiAgICAgICAgcm91bmQoYXZnKCJwcmljZSIpLCAyKS5hbGlhcygi5bmz5Z2H5L6h5qC8IikKICAgICkKICAgIC5vcmRlckJ5KCJjYXJhdF9jYXRlZ29yeSIsICJjdXQiKSAgICAgICAgICAgICMg44Kr44OG44K044Oq44CB44Kr44OD44OI44Gu6aCG44Gn44K944O844OICikKCnByaW50KCLwn5OKIOOCq+ODqeODg+ODiOOCq+ODhuOCtOODquODvOWIpeODu+OCq+ODg+ODiOWIpeOBruWIhuaekDoiKQpwcmludCgi77yI55Wq5Y+35LuY44GN44Op44OZ44Or44Gr44KI44KK44CB5bCP4oaS5Lit4oaS5aSn4oaS54m55aSn44Gu6aCG44Gn6KGo56S644GV44KM44G+44GZ77yJIikKcHJpbnQoIu+8iOOCteOCpOOCuuOBjOWkp+OBjeOBj+OBquOCi+OBu+OBqeS+oeagvOOBjOS4iuaYh+OBmeOCi+OBk+OBqOOBjOeiuuiqjeOBp+OBjeOBvuOBme+8iSIpCmRpc3BsYXkoY2F0ZWdvcnlfYW5hbHlzaXMp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView170a925\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView170a925\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView170a925\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView170a925) SELECT `carat_category`,`平均価格` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView170a925\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "可視化 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "carat_category",
             "id": "column_9bef94cf1266"
            },
            "y": [
             {
              "column": "平均価格",
              "id": "column_9bef94cf1267"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "scatter",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "平均価格": {
             "type": "scatter",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "d31aa9c1-b509-4aab-a6f6-3cdfad80d63b",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 28.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "carat_category",
           "type": "column"
          },
          {
           "column": "平均価格",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# カラットサイズでカテゴリ分け\n",
    "# when()関数で条件分岐を実装\n",
    "# 番号を付けることで、表示時に適切な順序でソートされるようにする\n",
    "diamonds_categorized = diamonds_clean.withColumn(\n",
    "    \"carat_category\",\n",
    "    when(col(\"carat\") < 0.5, \"1_小（< 0.5ct）\")\n",
    "    .when(col(\"carat\") < 1.0, \"2_中（0.5-1.0ct）\")\n",
    "    .when(col(\"carat\") < 1.5, \"3_大（1.0-1.5ct）\")\n",
    "    .otherwise(\"4_特大（>= 1.5ct）\")\n",
    ")\n",
    "\n",
    "# カテゴリとカットの組み合わせで集計\n",
    "category_analysis = (\n",
    "    diamonds_categorized\n",
    "    .groupBy(\"carat_category\", \"cut\")            # 2つの軸でグループ化\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"個数\"),\n",
    "        round(avg(\"price\"), 2).alias(\"平均価格\")\n",
    "    )\n",
    "    .orderBy(\"carat_category\", \"cut\")            # カテゴリ、カットの順でソート\n",
    ")\n",
    "\n",
    "print(\"📊 カラットカテゴリー別・カット別の分析:\")\n",
    "print(\"（番号付きラベルにより、小→中→大→特大の順で表示されます）\")\n",
    "print(\"（サイズが大きくなるほど価格が上昇することが確認できます）\")\n",
    "display(category_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60d8011-6b4f-4115-8c5f-ced083b8af36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ウィンドウ関数を使った高度な分析\n",
    "ウィンドウ関数を使うと、グループ内でのランキングや累積値などを計算できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6590cebe-f60b-4237-94d6-ce6b7465a970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank\n",
    "\n",
    "# ウィンドウの定義: カット品質ごとに価格でソート\n",
    "window_spec = Window.partitionBy(\"cut\").orderBy(col(\"price\").desc())\n",
    "\n",
    "# 各カット品質内での価格ランキングを計算\n",
    "top_diamonds = (\n",
    "    diamonds_clean\n",
    "    .withColumn(\"price_rank\", row_number().over(window_spec))  # 順位を付与\n",
    "    .filter(col(\"price_rank\") <= 3)                           # トップ3のみ抽出\n",
    "    .select(\"cut\", \"carat\", \"color\", \"clarity\", \"price\", \"price_rank\")\n",
    "    .orderBy(\"cut\", \"price_rank\")                              # 表示用にソート\n",
    ")\n",
    "\n",
    "print(\"🏆 各カット品質における最も高価なダイヤモンドTop3:\")\n",
    "display(top_diamonds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d47e14c1-4c0f-4e4e-a0df-f80899d3e38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🚀 第6部: SQLとの連携\n",
    "\n",
    "### Spark SQLの活用\n",
    "SparkデータフレームをSQLで操作する方法を学びます。\n",
    "SQLに慣れている方にとって、非常に親しみやすいインターフェースです。\n",
    "\n",
    "![](img/dataframe_sql.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9390b4eb-2f36-4993-8e2d-2a84b17f459c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# データフレームを一時ビューとして登録\n",
    "# これにより、SQLクエリでデータフレームを参照できるようになります\n",
    "diamonds_clean.createOrReplaceTempView(\"diamonds_view\")\n",
    "\n",
    "# SQLクエリを実行\n",
    "# spark.sql()メソッドでSQLを実行し、結果をデータフレームとして取得\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        cut,                                      -- カット品質\n",
    "        color,                                    -- 色のグレード\n",
    "        COUNT(*) as count,                        -- レコード数\n",
    "        ROUND(AVG(price), 2) as avg_price,       -- 平均価格\n",
    "        ROUND(AVG(carat), 3) as avg_carat        -- 平均カラット\n",
    "    FROM diamonds_view\n",
    "    WHERE price > 1000                           -- 1000ドル以上のダイヤモンドのみ\n",
    "    GROUP BY cut, color                          -- カットと色でグループ化\n",
    "    HAVING COUNT(*) > 100                        -- 100個以上のグループのみ\n",
    "    ORDER BY avg_price DESC                      -- 平均価格の降順\n",
    "    LIMIT 10                                      -- 上位10件\n",
    "\"\"\")\n",
    "\n",
    "print(\"🔍 SQL分析結果（価格1000ドル以上、100個以上のグループ）:\")\n",
    "display(sql_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33491a51-23b1-454f-8893-2b51d5e52f00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SQLマジックコマンドの使用\n",
    "Databricksでは`%sql`マジックコマンドを使って、セル全体をSQLクエリとして実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da737a8-2ed6-4455-a18e-751da529a45d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQLマジックコマンドで直接SQL実行\n",
    "-- より複雑な分析: 透明度ごとの統計\n",
    "SELECT \n",
    "    clarity,                                     -- 透明度グレード\n",
    "    COUNT(DISTINCT cut) as cut_variations,      -- カット種類の数\n",
    "    COUNT(*) as total_count,                    -- 総数\n",
    "    MIN(price) as min_price,                    -- 最低価格\n",
    "    MAX(price) as max_price,                    -- 最高価格\n",
    "    ROUND(MAX(price) - MIN(price), 2) as price_range  -- 価格レンジ\n",
    "FROM diamonds_view\n",
    "GROUP BY clarity\n",
    "ORDER BY total_count DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1583857a-8951-415a-94cb-09e9781debb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🎯 第7部: パフォーマンス最適化の基礎知識\n",
    "\n",
    "### 📚 最適化テクニックの理解\n",
    "\n",
    "Sparkのパフォーマンス最適化には以下のような手法があります：\n",
    "\n",
    "1. **キャッシング**: 頻繁に使用するデータフレームをメモリに保持\n",
    "2. **パーティショニング**: データを適切なサイズに分割\n",
    "3. **ブロードキャスト結合**: 小さなテーブルを各ノードに配布\n",
    "4. **述語プッシュダウン**: フィルタ条件を早期に適用\n",
    "\n",
    "### サーバレスコンピュートでの最適化\n",
    "\n",
    "Databricksのサーバレスコンピュートでは、多くの最適化が自動的に行われます。\n",
    "以下、実践可能な最適化テクニックを紹介します："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f62779d-6106-4774-9a50-72abb6078a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 最適化のベストプラクティスを実演\n",
    "\n",
    "# 1. フィルタの早期適用（述語プッシュダウン）\n",
    "# 良い例: フィルタを早期に適用\n",
    "optimized_df = (\n",
    "    diamonds_clean\n",
    "    .filter(col(\"price\") > 5000)      # まずフィルタ（データ量を削減）\n",
    "    .groupBy(\"cut\")                   # その後集計\n",
    "    .agg(avg(\"carat\").alias(\"avg_carat\"))\n",
    ")\n",
    "\n",
    "print(\"✅ 最適化された処理: フィルタを先に適用\")\n",
    "display(optimized_df)\n",
    "\n",
    "# 2. 必要なカラムのみ選択\n",
    "# データ転送量を削減\n",
    "selected_columns = (\n",
    "    diamonds_clean\n",
    "    .select(\"cut\", \"price\", \"carat\")  # 必要なカラムのみ選択\n",
    "    .filter(col(\"price\") > 10000)\n",
    ")\n",
    "\n",
    "print(\"\\n✅ 必要なカラムのみを選択して処理\")\n",
    "print(f\"選択されたカラム数: {len(selected_columns.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6b673a-53cb-4ad8-91fc-da326d65bfab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 最適化のベストプラクティス\n",
    "\n",
    "サーバレス環境で効果的な最適化手法：\n",
    "\n",
    "1. **早期フィルタリング**: WHERE句は可能な限り早い段階で適用\n",
    "2. **カラムの選択**: 必要なカラムのみをSELECT\n",
    "3. **適切な結合順序**: 小さいテーブルを先に結合\n",
    "4. **パーティション述語**: パーティション化されたテーブルでは適切なフィルタを使用\n",
    "\n",
    "![](img/optimization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07383692-2c03-494a-bbbc-18c055d76860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🔄 第8部: Pandas API on Sparkの活用\n",
    "\n",
    "### Pandas APIでSparkを使う\n",
    "Pandasに慣れている方でも、Sparkの分散処理能力を活用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f487c7bb-2550-4f98-bc32-eca7a302a078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "\n",
    "# Pandas API on Sparkでデータフレームを作成\n",
    "# 通常のPandasと同じ構文で記述できます\n",
    "ps_df = ps.DataFrame({\n",
    "    'A': np.random.rand(1000),     # ランダムな値（0-1）\n",
    "    'B': np.random.rand(1000),     # ランダムな値（0-1）\n",
    "    'C': np.random.randn(1000)     # 正規分布に従うランダム値\n",
    "})\n",
    "\n",
    "# Pandasと同じメソッドが使える\n",
    "print(\"📊 Pandas API on Sparkの統計情報:\")\n",
    "print(ps_df.describe())  # 基本統計量の表示\n",
    "\n",
    "print(\"\\n🔝 上位5件（Aカラムでソート）:\")\n",
    "# sort_valuesやheadなど、Pandasでお馴染みのメソッドが使用可能\n",
    "print(ps_df.sort_values('A', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2613d121-9475-4af4-a419-d022a1241f37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 既存のSparkデータフレームをPandas APIで操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1306d2e-624f-453f-9c97-90a7ffe1430e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SparkデータフレームをPandas API on Sparkに変換\n",
    "diamonds_ps = diamonds_clean.to_pandas_on_spark()\n",
    "\n",
    "# Pandasスタイルでの基本統計\n",
    "print(\"💰 価格カラムの統計情報（Pandas API）:\")\n",
    "price_stats = diamonds_ps['price'].describe()\n",
    "print(price_stats)\n",
    "\n",
    "# 基本的な集計を個別に実行\n",
    "print(\"\\n📈 カット別の基本統計:\")\n",
    "cut_types = diamonds_ps['cut'].unique().to_numpy()  # NumPy配列に変換\n",
    "for cut_type in cut_types[:3]:  # デモのため最初の3つのみ表示\n",
    "    cut_data = diamonds_ps[diamonds_ps['cut'] == cut_type]['price']\n",
    "    print(f\"\\n{cut_type}:\")\n",
    "    print(f\"  平均: ${cut_data.mean():.2f}\")\n",
    "    print(f\"  標準偏差: ${cut_data.std():.2f}\")\n",
    "    print(f\"  件数: {len(cut_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b177bc6d-206d-4f2e-b305-6ab1d5e2fa42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 💾 第9部: Unity Catalogとデータの永続化\n",
    "\n",
    "### 📚 Unity Catalogとは\n",
    "\n",
    "Unity Catalogは、Databricksのデータガバナンスソリューションです：\n",
    "- **統一されたメタストア**: すべてのデータ資産を一元管理\n",
    "- **細かいアクセス制御**: テーブル、カラムレベルでの権限管理\n",
    "- **データリネージ**: データの流れを追跡\n",
    "- **データ発見**: カタログを通じたデータの検索と理解\n",
    "\n",
    "### SparkとUnity Catalogの関係\n",
    "\n",
    "```\n",
    "Unity Catalog階層:\n",
    "├── カタログ (Catalog)\n",
    "│   ├── スキーマ/データベース (Schema/Database)\n",
    "│   │   ├── テーブル (Table)\n",
    "│   │   ├── ビュー (View)\n",
    "│   │   └── 関数 (Function)\n",
    "```\n",
    "\n",
    "![](img/uc.png)\n",
    "\n",
    "### データの保存先\n",
    "サーバレスコンピュートでは、ファイルシステムへの直接書き込みが制限されているため、\n",
    "Unity Catalogのテーブルとしてデータを保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8783c272-5b58-4b27-b6b3-162f773b360c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 現在のカタログとスキーマを確認\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "current_schema = spark.sql(\"SELECT current_database()\").collect()[0][0]\n",
    "\n",
    "print(\"📍 現在の位置:\")\n",
    "print(f\"  カタログ: {current_catalog}\")\n",
    "print(f\"  スキーマ: {current_schema}\")\n",
    "\n",
    "# workspace.defaultスキーマを使用\n",
    "# これは個人用の作業領域として提供されています\n",
    "spark.sql(\"USE workspace.default\")\n",
    "print(\"\\n✅ workspace.defaultスキーマに切り替えました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5713d09b-5265-4f7b-a03a-5d4370d86606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### マネージドテーブルとしての保存\n",
    "Unity Catalogの[マネージドテーブル](https://docs.databricks.com/aws/ja/tables/)としてデータを保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37767fda-165b-4097-86c0-81bef068238a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# ユニークなテーブル名を生成（衝突を避けるため）\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "unique_suffix = str(uuid.uuid4())[:8]\n",
    "table_name = f\"diamonds_analysis_{timestamp}_{unique_suffix}\"\n",
    "\n",
    "# テーブルとして保存\n",
    "# Delta形式で保存されます（Databricksのデフォルト）\n",
    "(\n",
    "    diamonds_clean.write\n",
    "    .mode(\"overwrite\")          # 既存テーブルがあれば上書き\n",
    "    .saveAsTable(f\"workspace.default.{table_name}\")\n",
    ")\n",
    "\n",
    "print(f\"✅ テーブルが保存されました: workspace.default.{table_name}\")\n",
    "print(\"\\n📊 保存されたテーブルの情報:\")\n",
    "spark.sql(f\"DESCRIBE EXTENDED workspace.default.{table_name}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e75c9b73-6858-48db-9f57-7e9c86cb7d17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "以下のセルを実行して表示されるリンクをクリックしてテーブルを確認してみましょう。Databricksでは[**カタログエクスプローラ**](https://docs.databricks.com/aws/ja/catalog-explorer/)というUIでテーブルやデータベース、ファイル、モデルなどにアクセスすることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51fc119-d017-4130-b56d-0873d8ee5fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "displayHTML(f\"<a href='/explore/data/workspace/default/{table_name}'>保存したテーブル</a>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eec2bf4-b5c5-49a3-8ac0-d50b7a5d4ae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 保存したテーブルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a7a354e-dc1d-4ca6-9417-40ca5fa60e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# テーブルからデータを読み込み\n",
    "loaded_df = spark.table(f\"workspace.default.{table_name}\")\n",
    "\n",
    "# 読み込んだデータを確認\n",
    "print(f\"📥 テーブルから読み込んだデータ:\")\n",
    "print(f\"  レコード数: {loaded_df.count():,}\")\n",
    "print(f\"  カラム数: {len(loaded_df.columns)}\")\n",
    "\n",
    "# SQLでもアクセス可能\n",
    "sql_query = f\"\"\"\n",
    "SELECT cut, COUNT(*) as count, AVG(price) as avg_price\n",
    "FROM workspace.default.{table_name}\n",
    "GROUP BY cut\n",
    "ORDER BY avg_price DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n🔍 SQLでテーブルをクエリ:\")\n",
    "display(spark.sql(sql_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2870300-cf80-4a27-8ea4-b7802e29ce52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Delta Lake形式の利点\n",
    "Databricksでは、テーブルはDelta Lake形式で保存されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190ad3d6-e53f-47d1-9a81-5fb30c564ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delta Lakeの特徴を確認\n",
    "print(\"🏞️ Delta Lake形式の利点:\")\n",
    "print(\"  1. ACID トランザクション: データの一貫性を保証\")\n",
    "print(\"  2. タイムトラベル: 過去のバージョンにアクセス可能\")\n",
    "print(\"  3. スキーマエボリューション: スキーマの変更に対応\")\n",
    "print(\"  4. データ圧縮: 効率的なストレージ使用\")\n",
    "print(\"  5. Z-Ordering: クエリパフォーマンスの最適化\")\n",
    "\n",
    "# テーブルの履歴を確認\n",
    "print(f\"\\n📜 テーブルの変更履歴:\")\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY workspace.default.{table_name} LIMIT 5\")\n",
    "display(history_df.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "548b5845-c5eb-4c02-a3bc-a8d641184742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🧹 第10部: クリーンアップとベストプラクティス\n",
    "\n",
    "### テーブルのクリーンアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b60534-aed1-4381-8874-2371300119c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 作成したテーブルを削除（もくもく会終了時）\n",
    "# コメントを外して実行してください\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS workspace.default.{table_name}\")\n",
    "# print(f\"🧹 テーブル {table_name} を削除しました\")\n",
    "\n",
    "print(\"⚠️ クリーンアップを実行する場合は、上記のコメントを外してください\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c365c0-8aae-4706-8969-a5f909d6c285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 📚 学んだことのまとめ\n",
    "\n",
    "このもくもく会で学んだ重要なポイント：\n",
    "\n",
    "#### 1. **Sparkの基本概念**\n",
    "   - 分散処理アーキテクチャ（ドライバーとエグゼキューター）\n",
    "   - 遅延評価（トランスフォーメーションとアクション）\n",
    "   - データフレームAPI\n",
    "   - Pandasとの違いと使い分け\n",
    "\n",
    "#### 2. **データ処理テクニック**\n",
    "   - データの読み込みとクレンジング\n",
    "   - 集計とグループ化\n",
    "   - ウィンドウ関数の活用\n",
    "\n",
    "#### 3. **SQL統合**\n",
    "   - SparkデータフレームのSQL操作\n",
    "   - 一時ビューの活用\n",
    "   - SQLマジックコマンド\n",
    "\n",
    "#### 4. **Unity Catalogとの統合**\n",
    "   - カタログ・スキーマ・テーブルの階層構造\n",
    "   - マネージドテーブルとしてのデータ保存\n",
    "   - Delta Lake形式の利点\n",
    "\n",
    "#### 5. **ベストプラクティス**\n",
    "   - フィルタの早期適用\n",
    "   - 必要なカラムのみ選択\n",
    "   - 適切なデータ形式の選択"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7beab8b-5793-4026-8799-afea021495e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🎓 次のステップ\n",
    "\n",
    "### さらに学習を深めるために\n",
    "\n",
    "1. [**構造化ストリーミング**](https://docs.databricks.com/aws/ja/structured-streaming/concepts): リアルタイムデータ処理\n",
    "2. [**MLlib**](https://docs.databricks.com/aws/ja/machine-learning/train-model/mllib): Sparkの機械学習ライブラリ\n",
    "3. [**Delta Lake**](https://docs.databricks.com/aws/ja/delta/): より高度なデータレイク機能\n",
    "4. [**Photon**](https://docs.databricks.com/aws/ja/compute/photon): Databricksの高速実行エンジン\n",
    "\n",
    "### 参考リソース\n",
    "- [Apache Spark公式ドキュメント](https://spark.apache.org/docs/latest/)\n",
    "- [Apache Spark徹底入門](https://www.amazon.co.jp/dp/B0CVQ84T6J/)\n",
    "- [Apache Sparkとは何か \\- Qiita](https://qiita.com/taka_yayoi/items/31190da754106b2d284e)\n",
    "- [PySparkことはじめ  \\- Qiita](https://qiita.com/taka_yayoi/items/a7ee6287031374efa88a)\n",
    "- [Databricks Learning](https://www.databricks.com/jp/learn)\n",
    "- [PySpark APIリファレンス](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Unity Catalogドキュメント](https://docs.databricks.com/ja/data-governance/unity-catalog/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d9c2cc-20cc-4744-87fa-ebfa02e16c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🚀 チャレンジ問題\n",
    "\n",
    "もくもく会の残り時間で以下の課題に挑戦してみましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba079b5b-b6fd-4204-8001-f2947f924754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 課題1: フィルタリングと集計\n",
    "カラットが1.0以上のダイヤモンドについて、カラー別・クラリティ別の平均価格を計算してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04738b6b-04db-40bf-9a6b-f79faff45fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ヒント: \n",
    "# 1. filter()でカラット >= 1.0のデータを抽出\n",
    "# 2. groupBy()で「color」と「clarity」でグループ化\n",
    "# 3. agg()で平均価格を計算\n",
    "# 4. orderBy()で結果をソート\n",
    "\n",
    "# あなたのコードをここに書いてください\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "109600c8-0da9-4cbb-b666-fb598768a360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 📝 課題1の解答例（クリックして展開）\n",
    "<details>\n",
    "<summary>解答を表示</summary>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0b33748-576f-4ab4-8fea-42e99e73ae01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 解答例1: DataFrame APIを使用\n",
    "from pyspark.sql.functions import avg, round, count\n",
    "\n",
    "large_diamonds_analysis = (\n",
    "    diamonds_clean\n",
    "    .filter(col(\"carat\") >= 1.0)                # カラット1.0以上をフィルタ\n",
    "    .groupBy(\"color\", \"clarity\")                # カラーと透明度でグループ化\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"個数\"),\n",
    "        round(avg(\"price\"), 2).alias(\"平均価格\")  # 平均価格を計算\n",
    "    )\n",
    "    .orderBy(\"平均価格\", ascending=False)        # 平均価格の降順でソート\n",
    ")\n",
    "\n",
    "print(\"💎 1カラット以上のダイヤモンド分析結果:\")\n",
    "display(large_diamonds_analysis.limit(20))\n",
    "\n",
    "# 解答例2: SQLを使用\n",
    "sql_solution1 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        color,\n",
    "        clarity,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(AVG(price), 2) as avg_price\n",
    "    FROM diamonds_view\n",
    "    WHERE carat >= 1.0\n",
    "    GROUP BY color, clarity\n",
    "    ORDER BY avg_price DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🔍 SQL版の結果:\")\n",
    "display(sql_solution1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e306a7-5d77-4b39-8034-962903b7017a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 課題2: ウィンドウ関数で中央値を見つける\n",
    "各カラーグループ内で、価格が中央値に最も近いダイヤモンドを見つけてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20952d5f-0cc0-4872-bd1f-e1c96b88602a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ヒント:\n",
    "# 1. 各カラーグループの中央値を計算\n",
    "# 2. ウィンドウ関数でランク付け\n",
    "# 3. 中央値との差が最小のレコードを抽出\n",
    "\n",
    "# あなたのコードをここに書いてください\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f18f613e-17cf-4ced-b8bd-58936555426e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 📝 課題2の解答例（クリックして展開）\n",
    "<details>\n",
    "<summary>解答を表示</summary>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2446feff-d543-4f88-83b1-95b3a7633acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 解答例: ウィンドウ関数を使った中央値の近似\n",
    "from pyspark.sql.functions import percentile_approx, abs, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: 各カラーグループの中央値を計算\n",
    "color_median = (\n",
    "    diamonds_clean\n",
    "    .groupBy(\"color\")\n",
    "    .agg(percentile_approx(\"price\", 0.5).alias(\"median_price\"))\n",
    ")\n",
    "\n",
    "# Step 2: 元のデータと中央値をJOIN\n",
    "diamonds_with_median = (\n",
    "    diamonds_clean\n",
    "    .join(color_median, on=\"color\")\n",
    "    .withColumn(\"diff_from_median\", abs(col(\"price\") - col(\"median_price\")))  # 中央値との差\n",
    ")\n",
    "\n",
    "# Step 3: ウィンドウ関数で各カラー内でランク付け\n",
    "window_median = Window.partitionBy(\"color\").orderBy(\"diff_from_median\")\n",
    "\n",
    "closest_to_median = (\n",
    "    diamonds_with_median\n",
    "    .withColumn(\"rank\", row_number().over(window_median))\n",
    "    .filter(col(\"rank\") == 1)  # 最も中央値に近いものを選択\n",
    "    .select(\"color\", \"carat\", \"cut\", \"clarity\", \"price\", \"median_price\", \"diff_from_median\")\n",
    "    .orderBy(\"color\")\n",
    ")\n",
    "\n",
    "print(\"🎯 各カラーグループで価格が中央値に最も近いダイヤモンド:\")\n",
    "display(closest_to_median)\n",
    "\n",
    "# 別解: SQL版\n",
    "sql_solution2 = spark.sql(\"\"\"\n",
    "    WITH color_median AS (\n",
    "        SELECT \n",
    "            color,\n",
    "            PERCENTILE_APPROX(price, 0.5) as median_price\n",
    "        FROM diamonds_view\n",
    "        GROUP BY color\n",
    "    ),\n",
    "    ranked_diamonds AS (\n",
    "        SELECT \n",
    "            d.*,\n",
    "            m.median_price,\n",
    "            ABS(d.price - m.median_price) as diff_from_median,\n",
    "            ROW_NUMBER() OVER (PARTITION BY d.color ORDER BY ABS(d.price - m.median_price)) as rn\n",
    "        FROM diamonds_view d\n",
    "        JOIN color_median m ON d.color = m.color\n",
    "    )\n",
    "    SELECT \n",
    "        color, carat, cut, clarity, price, median_price, diff_from_median\n",
    "    FROM ranked_diamonds\n",
    "    WHERE rn = 1\n",
    "    ORDER BY color\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🔍 SQL版の結果:\")\n",
    "display(sql_solution2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc25df17-778b-48c2-9425-06d8086345fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 課題3: 複雑なJOINと分析\n",
    "カット別とカラー別の統計を別々に計算し、JOINして最も高価な組み合わせを見つけてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9565b037-02fe-4e98-813a-85716d930e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ヒント:\n",
    "# 1. カット別の平均価格データフレームを作成\n",
    "# 2. カラー別の平均価格データフレームを作成  \n",
    "# 3. 両方のデータフレームをクロスJOIN\n",
    "# 4. 組み合わせごとの予想価格を計算\n",
    "\n",
    "# あなたのコードをここに書いてください\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f8e3ed4-9f16-4854-86f8-155b7eb0d8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 📝 課題3の解答例（クリックして展開）\n",
    "<details>\n",
    "<summary>解答を表示</summary>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c3ef678-ebdd-4e4b-ba22-73d3887c01ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 解答例: 複雑なJOINと分析\n",
    "from pyspark.sql.functions import avg, round, count, max as spark_max\n",
    "\n",
    "# Step 1: カット別の統計\n",
    "cut_stats = (\n",
    "    diamonds_clean\n",
    "    .groupBy(\"cut\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"cut_count\"),\n",
    "        round(avg(\"price\"), 2).alias(\"cut_avg_price\"),\n",
    "        round(avg(\"carat\"), 3).alias(\"cut_avg_carat\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 2: カラー別の統計\n",
    "color_stats = (\n",
    "    diamonds_clean\n",
    "    .groupBy(\"color\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"color_count\"),\n",
    "        round(avg(\"price\"), 2).alias(\"color_avg_price\"),\n",
    "        round(avg(\"carat\"), 3).alias(\"color_avg_carat\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 3: 実際の組み合わせの統計\n",
    "actual_combination = (\n",
    "    diamonds_clean\n",
    "    .groupBy(\"cut\", \"color\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"actual_count\"),\n",
    "        round(avg(\"price\"), 2).alias(\"actual_avg_price\"),\n",
    "        spark_max(\"price\").alias(\"max_price\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 4: すべてをJOINして分析\n",
    "combined_analysis = (\n",
    "    actual_combination\n",
    "    .join(cut_stats, on=\"cut\")\n",
    "    .join(color_stats, on=\"color\")\n",
    "    .withColumn(\n",
    "        \"predicted_price\",\n",
    "        round((col(\"cut_avg_price\") + col(\"color_avg_price\")) / 2, 2)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"price_difference\",\n",
    "        round(col(\"actual_avg_price\") - col(\"predicted_price\"), 2)\n",
    "    )\n",
    "    .select(\n",
    "        \"cut\", \"color\",\n",
    "        \"actual_count\",\n",
    "        \"actual_avg_price\",\n",
    "        \"predicted_price\",\n",
    "        \"price_difference\",\n",
    "        \"max_price\"\n",
    "    )\n",
    "    .orderBy(col(\"actual_avg_price\").desc())\n",
    ")\n",
    "\n",
    "print(\"💰 最も高価な組み合わせ Top 10:\")\n",
    "display(combined_analysis.limit(10))\n",
    "\n",
    "# 追加分析: 予測と実際の差が大きい組み合わせ\n",
    "surprising_combinations = (\n",
    "    combined_analysis\n",
    "    .orderBy(abs(col(\"price_difference\")).desc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "print(\"\\n😮 予測と実際の価格差が大きい組み合わせ:\")\n",
    "display(surprising_combinations)\n",
    "\n",
    "# SQL版の解答\n",
    "sql_solution3 = spark.sql(\"\"\"\n",
    "    WITH cut_stats AS (\n",
    "        SELECT \n",
    "            cut,\n",
    "            COUNT(*) as cut_count,\n",
    "            ROUND(AVG(price), 2) as cut_avg_price\n",
    "        FROM diamonds_view\n",
    "        GROUP BY cut\n",
    "    ),\n",
    "    color_stats AS (\n",
    "        SELECT \n",
    "            color,\n",
    "            COUNT(*) as color_count,\n",
    "            ROUND(AVG(price), 2) as color_avg_price\n",
    "        FROM diamonds_view\n",
    "        GROUP BY color\n",
    "    ),\n",
    "    actual_combo AS (\n",
    "        SELECT \n",
    "            cut,\n",
    "            color,\n",
    "            COUNT(*) as actual_count,\n",
    "            ROUND(AVG(price), 2) as actual_avg_price,\n",
    "            MAX(price) as max_price\n",
    "        FROM diamonds_view\n",
    "        GROUP BY cut, color\n",
    "    )\n",
    "    SELECT \n",
    "        a.cut,\n",
    "        a.color,\n",
    "        a.actual_count,\n",
    "        a.actual_avg_price,\n",
    "        ROUND((cs.cut_avg_price + co.color_avg_price) / 2, 2) as predicted_price,\n",
    "        a.max_price,\n",
    "        ROUND(a.actual_avg_price - (cs.cut_avg_price + co.color_avg_price) / 2, 2) as price_diff\n",
    "    FROM actual_combo a\n",
    "    JOIN cut_stats cs ON a.cut = cs.cut\n",
    "    JOIN color_stats co ON a.color = co.color\n",
    "    ORDER BY a.actual_avg_price DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🔍 SQL版の最も高価な組み合わせ:\")\n",
    "display(sql_solution3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "423fd598-08b8-43cd-85a3-8112aa2a96ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📝 振り返りメモ\n",
    "\n",
    "今日学んだことを記録しておきましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bebdd8e-61d1-464f-b339-5c64abd5c803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 今日の学び（自由に編集してください）\n",
    "my_learnings = \"\"\"\n",
    "1. Sparkの基本概念：\n",
    "   - 分散処理の仕組みが理解できた\n",
    "   - 遅延評価の重要性がわかった\n",
    "   \n",
    "2. 特に面白かった機能：\n",
    "   - ウィンドウ関数での高度な分析\n",
    "   - SQLとDataFrame APIの連携\n",
    "   \n",
    "3. 実務で使えそうなテクニック：\n",
    "   - 大規模データの集計処理\n",
    "   - Unity Catalogでのデータ管理\n",
    "   \n",
    "4. もっと深く学びたいトピック：\n",
    "   - 構造化ストリーミング\n",
    "   - 機械学習との連携\n",
    "\"\"\"\n",
    "\n",
    "print(my_learnings)\n",
    "\n",
    "# メモを保存したい場合は、以下のコメントを外してテーブルとして保存\n",
    "# spark.sql(f\"CREATE OR REPLACE TABLE workspace.default.my_learning_memo AS SELECT '{my_learnings}' as memo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac4d0ce5-0a62-410a-bec6-76926561d54a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🎉 お疲れさまでした！\n",
    "\n",
    "Apache Sparkの世界への第一歩を踏み出しました。今日学んだ基礎を活かして、より大規模で複雑なデータ処理に挑戦していってください！\n",
    "\n",
    "### 💬 質問とディスカッション\n",
    "- 不明な点があれば、遠慮なくメンターに質問してください\n",
    "- 他の参加者と学びを共有しましょう\n",
    "- 実務での活用アイデアを話し合いましょう\n",
    "\n",
    "### 🔗 コミュニティ\n",
    "- Databricks Community に参加して継続的に学習\n",
    "- Apache Sparkユーザーグループでの情報交換\n",
    "- 定期的なもくもく会への参加\n",
    "\n",
    "**Happy Sparking! ✨**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8447732533051704,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-mokumoku-notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
